Kubernetes
From:  https://youtu.be/X48VuDVv0do





Kubernetes:  Open source container orchestration tool
  - developed by google
  - manage containerized applications in different deployment environments
    - physical
    - cloud
    - virtual

containers + microservices >> need to manage hundreds of containers

Kubernetes features:
 - High Availability - no downtime 
 - Scalability - high performance 
 - Disaster Recovery - backup and restore 



######################################################################################
## Kubernetes Components
######################################################################################

Node:  Server, physical or virtual machine

Pod:   Smallest unit of K8s
        - An abstraction over a container
        - pod creates a layer over container (running environment)
        - Kubernetes wants to "abstract away" container runtime
            - ability to replace container tech
        - ONLY interact with Kubernetes Layer (not docker)

NOTE: Pod usually just runs one application inside of it but can run multiple apps in one pod.

    Each Pod gets its own IP address (internal)
        - each pod can communicate with each other utilizing Pod IP Address

    NOTE: pods are ephenemeral and can be killed an spun up.  A new pod will get a new IP address.

        +-Node----------------------------+
        |                                 |
        |        +Pod-------------+       |
        |        |                |       |
        |        |  Application   |       |
        |        |                |-[IP]  |
        |        |[--abstraction-]|  |    |
        |        |  [ container ] |  |    |
        |        +----------------+  |    |
        |                            |    |
        |        +Pod-------------+  |    |
        |        |                |  |    |
        |        |    Database    |  |    |
        |        |                |-[IP]  |
        |        |[--abstraction-]|       |
        |        |  [ container ] |       |
        |        +----------------+       |
        |                                 |
        +---------------------------------+

Service:  Permanent (static) IP Address that can be attached to a pod
        - Life cycle of Pod and Service are NOT connecited

    ex. application can communicate with DB without updating 
        connection if DB pod is restarted

External Service:  Available via web browser, etc
        Utilize Ingress to  map 
        http://124.89.101.2:8080 to https://myapp.com

        Utilize Ingres, routes traffic into cluster

Internal Service:  No external access, service only for other k8s services / objects

        +-Node----------------------------+
        |                                 |
        |    [Secret] [ConfigMap]         |
        |       |          |              |
        |       +----+-----+    [Ingres]--<------- 
        |            |              |     |
        |    +Pod-------------+     |     |
        |    |                |     |     |
        |    |  Application   |     |     |
        |    |                |-[Service] |
        |    |[--abstraction-]|     |     |
        |    |  [ container ] |     |     |
        |    +----------------+     |     |
        |                           |     |
        |    +Pod-------------+     |     |
        |    |                |     |     |
        |    |    Database    |     |     |
        |    |                |-[Service] |
        |    |[--abstraction-]|           |
        |    |  [ container ] |           |
        |    +----------------+           |
        |                                 |
        +---------------------------------+


# ConfigMap / Secret

ConfigMap: external configuration of your application, connect to pod
           contains configuration information needed by app i.e. db connect string
            DB_URL=mongodb

Secret: like configmap but used to store secret data i.e. credentials / certificates
        connect to pod
        - base64 encoded i.e. username / password
            DB_USER=username
            DB_PWD =password

        - Use as ENV vars or properties file

    NOTE: built-in security mechanism is not enabled by default


# Volumes

Volumes:  Persist data (logs, db, etc) beyond the life of a pod.
          Attaches physical hard drive storage to a pod
            - local machine (node storage where pod is located)
            - remote storage (outside of k8s cluster)

        +-Node----------------------------+
        |                                 |
        |    +Pod-------------+           |
        |    |                |           |
        |    |  Application   |           |
        |    |                |-[Service] |
        |    |[--abstraction-]|     |     |
        |    |  [ container ] |     |     |
        |    +----------------+     |     |
        |                           |     |
        |    +Pod-------------+     |     |
        |    |                |     |     |
        |    |    Database    |     |     |
        |    |                |-[Service] |
        |    |[--abstraction-]|           |
        |    |  [ container ] |           |
        |    +----------------+           |
        |         [Volume]----------------+--------[remote ]
        |            |                    |        [storage]
        |            +----[ local ]       |
        |                 [storage]       |
        |                                 |
        +---------------------------------+

NOTE: K8s Does Not Manage Data Persistence


# Deployment / StatefulSet

Deployment:  blueprint of a pod, specify how many replicas
        - Deployment is an abstraction on pods
            Interaction done with Deployments and not Pods directly
        - Can scale up / down number of replicas needed
        - Replica is connected to same Service
        - Service has 2 functions:
            - permanent IP
            - load balancer 



        +------------------------+        +------------------------+
        |              [[--------Deployment--------]]              |
        |               |        |        |        |               |
        |    +Pod-------------+  |        |  +Pod-------------+    |
        |    |                |  |        |  |                |    |
        |    |  Application   |  |        |  |  Application   |    |
        |    |                |---[Service]--|                |    |
        |    +----------------+  |        |  +----------------+    |
        |                        |        |                        |
        |    +Pod-------------+  |        |  +Pod-------------+    |
        |    |                |  |        |  |                |    |
        |    |    Database    |  |        |  |    Database    |    |
        |    |                |--[Service]---|                |    |
        |    +----------------+  |        |  +----------------+    |
        |               |        |        |        |               |
        |              [[------StatefulSet---------]]              |
        |                        |        |                        |
        +--Node-1----------------+        +--Node-2----------------+



NOTE: Databases can't be replicated via Deployment because
      Database has State.  Need to avoid data inconsistencies
      hence StatefulSet

StatefulSet:  Used for Stateful applications i.e. databases

    Deployments  -->>  Apps 
    StatefulSets -->>  Databases 


StatefulSet responsible for scaling up / down pods but importatntly
makes sure the db reads / writes are synchronized 

NOTE: Deploying StatefulSets can be difficult 
      Databases are often hosed outside of K8s cluster


######################################################################################
## Kubernetes Architecture
######################################################################################



Two types of nodes:  Master / Workder (Node)
  - Each Node has multiple pods on it 
  - 3 Processes MUST BE INSTALLED ON EVERY Node to:
    - manage and schedule pods

Nodes to the "work" i.e. Worker Nodes

Processes needed: 
1. Container Runtime
    - independent of Kubernetes

2. Kubelet: 
    - Kubernetes process
    - process that schedules Pods/containers
    - interacts with both container and node
    - Kubelet starts the pod with a container inside
        - assigns resources from node to container
            - i.e. CPU / RAM / Storage

3. Kube proxy: 
    - Kubernetes process
    - forwards requests from service to a specific pod
        - intelligent forwarding logic
        - communication performant and low overhead
            i.e. App requesting DB Service will get routed to same node 
                if both App and DB are running on same node
                    - no network overhead

        +-Node-----------------------------+
        |                                  |
        |    +Pod-------------+            |
        |    |                |            |
        |    |     App        |------+     |
        |    |                |      |     |
        |    +----------------+  [ Kube ]<-+------>>
        |                        [Proxy ]  |
        |    +Pod-------------+      |     |
        |    |                |      |     |
        |    |    Database    |------+     |
        |    |                |      |     |
        |    +----------------+      |     |
        |                            |     |
        |                            |     |
        |                       [Kubelet]  |
        |                           |      |
        |[container runtime]--------+      |
        |  ( Docker )                      |
        +----------------------------------+

Master Nodes: 
  - schedules pod
  - reschedule / restarts pod
  - joins new Node
  - 4 Processes MUST BE INSTALLED ON EVERY Master Node

1. API Server 
  - Client interacts with API server to deploy new app, etc
        - kubelet, UI, API
  - Cluster Gateway 
    - Updates, Queries
  - Gatekeeper for Authentication

2. Scheduler
  - Request for a new Pod: API Server -->> Scheduler
    - starts app on worker node
    - determines where (which node) pod should be schedule
    - initiates scheduling thru kubelet

3. Controller Manager
  - Detects state changes of cluster (i.e. crashed pods)
    - reschedules dead pods i.e. restore state
        - Controller Manager -> Scheduler -> Kubelet

4. etcd
  - key value store of Cluster State
    - "cluster brain"
    - All cluster changes stored in key value store
        - killed / restarted pod
        

NOTE: Application data NOT stored in etcd        
                                                                          [Client]
                                                                             |
                                                                      Update | Query
        +-Node-----------------------------+               +-Master--------------------------------+           
        |                                  |               |    [-----   API Server   -----]       |  <-- Cluster Gateway
        |    +Pod-------------+            |               |                                       |            + 
        |    |                |            |               |                                       |       Auth / Auth
        |    |     App        |------+     |          +----+----[-----   Scheduler   ------] <-+   | 
        |    |                |      |     |          |    |                ^                  |   |
        |    +----------------+  [ Kube ]<-+---->>    |    |        restart | crashed pod      |   |
        |                        [Proxy ]  |          |    |                |                  |   |
        |    +Pod-------------+      |     |          |    |    [--- Controller Manager ---] <-+   |
        |    |                |      |     |          |    |                                   |   |
        |    |    Database    |------+     |          |    |                                   |   |
        |    |                |      |     |          |    |                                   |   |
        |    +----------------+      |     |          |    |    [---------- etcd ----------] <-+   |  <-- Cluster "Brain"
        |                            |     |          |    |    [ key / value, key / value ]       |   holds state of cluster for
        |                            |     | request  |    |                                       |  Controller Manager and Scheduler
        |                       [Kubelet]<-+----------+    |                                       |
        |                           |      | new pod       |                                       |
        |[container runtime]--------+      |               |                                       |
        |  ( Docker )                      |               |                                       |
        +----------------------------------+               +---------------------------------------+           


Masters are usually in a cluster
 - API Server is load balanced
 - etcd is distributed storage across all master nodes 

        +-Master-1--------+    +-Master-2--------+    +-Master-3--------+
        |                 |    |                 |    |                 |
   +----+-----------------+----+-----------------+----+-----------------+----+
   |    |                 |    |                 |    |                 |    |
   |    | [ API Server ]  |    | [ API Server ]  |    | [ API Server ]  |    | <-- Load Balanced across Masters
   |    |                 |    |                 |    |                 |    |
   +----+-----------------+----+-----------------+----+-----------------+----+
        |                 |    |                 |    |                 |
        | [ Scheduler  ]  |    | [ Scheduler  ]  |    | [ Scheduler  ]  |
        |                 |    |                 |    |                 |
        | [Controller Mgr]|    | [Controller Mgr]|    | [Controller Mgr]|
        |                 |    |                 |    |                 |
   +----+-----------------+----+-----------------+----+-----------------+----+
   |    |                 |    |                 |    |                 |    |
   |    | [    etcd    ]  |    | [    etcd    ]  |    | [    etcd    ]  |    | <-- Distributed Storage across Masters
   |    |                 |    |                 |    |                 |    |
   +----+-----------------+----+-----------------+----+-----------------+----+
        |                 |    |                 |    |                 |
        +-----------------+    +-----------------+    +-----------------+


Master Nodes need less resources:
    - CPU / RAM / Storage
    - they are only running master processes

Worker Nodes need MORE resources:
    - CPU / RAM / Storage
    - they are running the applications and interfacing with pods


Increasing cluster with new Master / Nodes:
    - get new bare server
    - install master / worker node processes
    - add to cluster




######################################################################################
## minikube and kubectl - local setup
######################################################################################

minikube: one node cluster 
  - master process and worker process on one machine 
  - docker pre-installed
  - run via virtual box
  - 1 node k8s cluster that runs in VirutalBox
        - testing, etc.

        Minikube
 +--------------------------+
 |  +--------------------+  |
 |  | [Master Processes] |  |
 |  |                    |  |
 |  | [Worker Processes] |  |
 |  |                    |  |
 |  |                    |  |
 |  | [    Docker     ]  |  |
 |  +--------Node--------+  |
 |                          |
 +-VirtualBox---------------+


NOTE: check this out for minikube on lxc:
      https://github.com/d3adwolf/kubernetes-inside-proxmox-lxc

kubectl
 - interact with k8s cluster via API Server
    - UI, API, CLI
 - kubectl is CLI client 
 - kubectl can interface with minikube and k8s clusters


    [ minikube ]   [ k8s cluster ]
        +-----------------+
                | 
             kubectl


Install doc:
https://kubernetes/io/docs/tasks/tools/install-minikube
https://kubernetes/io/docs/tasks/tools/install-kubectl


Install minikube on a lxc container on proxmox:
https://github.com/d3adwolf/kubernetes-inside-proxmox-lxc


NOTE: minikube is installed on an Ubuntu 24 image on proxmox as minikube at 192.168.99.33 w/ ansible id installed
ssh ansible@minikube
Latest minikube and kubectl installed as well as docker

kubectl get nodes
NAME       STATUS   ROLES           AGE     VERSION
minikube   Ready    control-plane   2m50s   v1.32.0

minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.0

######################################################################################
## kubectl commands
######################################################################################

# get status of nodes
kubectl get nodes
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   47h   v1.32.0


# get pods (none deploye, hence no resources)
kubectl get pods
No resources found in default namespace.

# get services 
kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   47h


Now let's create a pod 

kubectl create -h

Create a resource from a file or from stdin.

 JSON and YAML formats are accepted.

Examples:
  # Create a pod using the data in pod.json
  kubectl create -f ./pod.json
  
  # Create a pod based on the JSON passed into stdin
  cat pod.json | kubectl create -f -
  
  # Edit the data in registry.yaml in JSON then create the resource using the edited data
  kubectl create -f registry.yaml --edit -o json


######################################################################################
## Create a Deployment (and indirectly a Replicaset and Pod)
######################################################################################

Can't create pod directly, Pod is the smallest unit within the cluster.
Deployment is an abstraction layer over Pods

  kubectl create deployment NAME --image=image -- [COMMAND] [args...] [options]

keys are NAME and image.  Pod is created from an image

i.e. 
kubectl create deployment nginx-dep --image=nginx   ## This deploys and nginx docker image
> deployment.apps/nginx-dep created

kctl get deployment                                 ## now let's get the deployments, FYI I made an alias kctl=kubectl
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
nginx-dep   1/1     1            1           70s


We successfully executed the deployment, let's check on the underlying pod

kctl get pod                                        ## get the pod, under the deployment
NAME                      READY   STATUS    RESTARTS   AGE
nginx-dep-5879cc9-gszq9   1/1     Running   0          2m26s


A deployment is the blueprint for creating a pod
Most basic config for deployment: 
    - name and image to use   (rest are defaults)


Between a deployment and a pod there is another layer that is automatically managed by deployment:  ReplicaSet

kctl get replicaset                                 ## get the ReplicaSet, notice the name is a subset of the pod or vice versa
NAME                DESIRED   CURRENT   READY   AGE
nginx-dep-5879cc9   1         1         1       6m19s


Replicaset manages the replicas of a Pod
NOTE: you do not interact directly with replicasets, only deal with deployments

In the deployment you can configure howmany replicas of pod to deploy


Abstraction Layer 


        [ Deployment manages .. ]
      [ ReplicaSet  manages   .. ]          + -- k8s manages from here down -- +
     [ Pod is an abstraction of .. ]        |                                  | 
   [ container ]         [ container ]      v                                  v


Everything below deployment is handeled by k8s

######################################################################################
## Edit / Change a Deployment (and indirectly a Replicaset and Pod)
######################################################################################

Now let's edit the deployment 
kubectl edit deployment nginx-dep           # provides and configuration file that represents the deployment

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2025-01-31T02:23:10Z"
  generation: 1
  labels:
    app: nginx-dep
  name: nginx-dep
  namespace: default
  resourceVersion: "9640"
  uid: f383eaf6-a15a-4b5a-9f7b-4a41e49fedc3
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx-dep
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-dep
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2025-01-31T02:23:15Z"
    lastUpdateTime: "2025-01-31T02:23:15Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2025-01-31T02:23:10Z"
    lastUpdateTime: "2025-01-31T02:23:15Z"
    message: ReplicaSet "nginx-dep-5879cc9" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
                         

NOTE:   change image to nginx:1.16 and save deployment file
        - image: nginx:1.16
Note below that the pod and replicaset have been updated

kctl edit deployment nginx-dep
deployment.apps/nginx-dep edited

kctl get deployment
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
nginx-dep   1/1     1            1           20m

kctl get pods
NAME                         READY   STATUS    RESTARTS   AGE
nginx-dep-579d65b68f-7zn5w   1/1     Running   0          21s


kctl get replicaset                     ## Hey the replicaset count of the prior version is now 0 !!
NAME                   DESIRED   CURRENT   READY   AGE
nginx-dep-579d65b68f   1         1         1       35s
nginx-dep-5879cc9      0         0         0       20m

HENCE, by make one change to the deployment, via the generated deployment file, the replicaset and pods are updated 

######################################################################################
## Pod Debugging
######################################################################################

kubectl logs [pod name]

# first let's create a deployment that will generate some logs
kctl create deployment mong-deply --image=mongo
deployment.apps/mong-deply created



kctl get deployment
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
mong-deply   0/1     1            0           38s
nginx-dep    1/1     1            1           31m

kctl get replicaset
NAME                    DESIRED   CURRENT   READY   AGE
mong-deply-659b9dd647   1         1         0       47s
nginx-dep-579d65b68f    1         1         1       11m
nginx-dep-5879cc9       0         0         0       31m

kctl get pods
NAME                          READY   STATUS             RESTARTS      AGE
mong-deply-659b9dd647-9wqtv   0/1     CrashLoopBackOff   2 (25s ago)   52s
nginx-dep-579d65b68f-7zn5w    1/1     Running            0             11m



######################################################################################
## Get Logs from Pods  (sorta rhymes)
######################################################################################

kctl logs mong-deply-659b9dd647-9wqtv
ansible@minikube:~$ kctl logs mong-deply-659b9dd647-9wqtv
{"t":{"$date":"2025-01-31T03:08:19.785+00:00"},"s":"I",  "c":"CONTROL",  "id":23285,   "ctx":"main","msg":"Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'"}
{"t":{"$date":"2025-01-31T03:08:19.786+00:00"},"s":"I",  "c":"CONTROL",  "id":5945603, "ctx":"main","msg":"Multi threading initialized"}
{"t":{"$date":"2025-01-31T03:08:19.789+00:00"},"s":"I",  "c":"NETWORK",  "id":4648601, "ctx":"main","msg":"Implicit TCP FastOpen unavailable. If TCP FastOpen is required, set at least one of the related parameters","attr":{"relatedParameters":["tcpFastOpenServer","tcpFastOpenClient","tcpFastOpenQueueSize"]}}
{"t":{"$date":"2025-01-31T03:08:19.790+00:00"},"s":"I",  "c":"NETWORK",  "id":4915701, "ctx":"main","msg":"Initialized wire specification","attr":{"spec":{"incomingExternalClient":{"minWireVersion":0,"maxWireVersion":25},"incomingInternalClient":{"minWireVersion":0,"maxWireVersion":25},"outgoing":{"minWireVersion":6,"maxWireVersion":25},"isInternalClient":true}}}
{"t":{"$date":"2025-01-31T03:08:19.791+00:00"},"s":"I",  "c":"TENANT_M", "id":7091600, "ctx":"main","msg":"Starting TenantMigrationAccessBlockerRegistry"}
{"t":{"$date":"2025-01-31T03:08:19.791+00:00"},"s":"I",  "c":"CONTROL",  "id":4615611, "ctx":"initandlisten","msg":"MongoDB starting","attr":{"pid":1,"port":27017,"dbPath":"/data/db","architecture":"64-bit","host":"mong-deply-659b9dd647-9wqtv"}}
{"t":{"$date":"2025-01-31T03:08:19.791+00:00"},"s":"I",  "c":"CONTROL",  "id":23403,   "ctx":"initandlisten","msg":"Build Info","attr":{"buildInfo":{"version":"8.0.4","gitVersion":"bc35ab4305d9920d9d0491c1c9ef9b72383d31f9","openSSLVersion":"OpenSSL 3.0.13 30 Jan 2024","modules":[],"allocator":"tcmalloc-google","environment":{"distmod":"ubuntu2404","distarch":"x86_64","target_arch":"x86_64"}}}}
{"t":{"$date":"2025-01-31T03:08:19.791+00:00"},"s":"I",  "c":"CONTROL",  "id":51765,   "ctx":"initandlisten","msg":"Operating System","attr":{"os":{"name":"Ubuntu","version":"24.04"}}}
{"t":{"$date":"2025-01-31T03:08:19.791+00:00"},"s":"I",  "c":"CONTROL",  "id":21951,   "ctx":"initandlisten","msg":"Options set by command line","attr":{"options":{"net":{"bindIp":"*"}}}}
{"t":{"$date":"2025-01-31T03:08:19.792+00:00"},"s":"I",  "c":"STORAGE",  "id":22297,   "ctx":"initandlisten","msg":"Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem","tags":["startupWarnings"]}
{"t":{"$date":"2025-01-31T03:08:19.792+00:00"},"s":"I",  "c":"STORAGE",  "id":22315,   "ctx":"initandlisten","msg":"Opening WiredTiger","attr":{"config":"create,cache_size=1446M,session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,remove=true,path=journal,compressor=snappy),builtin_extension_config=(zstd=(compression_level=6)),file_manager=(close_idle_time=600,close_scan_interval=10,close_handle_minimum=2000),statistics_log=(wait=0),json_output=(error,message),verbose=[recovery_progress:1,checkpoint_progress:1,compact_progress:1,backup:0,checkpoint:0,compact:0,evict:0,history_store:0,recovery:0,rts:0,salvage:0,tiered:0,timestamp:0,transaction:0,verify:0,log:0],prefetch=(available=true,default=false),"}}





######################################################################################
## Now Describe That Pod
######################################################################################

# now describe the pod          NOTE: This shows alot of detail about pod
kctl describe pod [pod name]

kctl describe pod mong-deply-659b9dd647-9wqtv
Name:             mong-deply-659b9dd647-9wqtv
Namespace:        default
Priority:         0
Service Account:  default
Node:             minikube/192.168.49.2
Start Time:       Fri, 31 Jan 2025 02:53:39 +0000
Labels:           app=mong-deply
                  pod-template-hash=659b9dd647
Annotations:      <none>
Status:           Running
IP:               10.244.0.18
IPs:
  IP:           10.244.0.18
Controlled By:  ReplicaSet/mong-deply-659b9dd647
Containers:
  mongo:
    Container ID:   docker://10e76f62f509c6863346a4c1bcaf88dce36f1b0534a1bcf01bf826081978e972
    Image:          mongo
    Image ID:       docker-pullable://mongo@sha256:c7ac28ef4d8137358ed86014a9d10dda2730a64046ce2a49610ad4bd9788d4cb
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Fri, 31 Jan 2025 03:08:19 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    132
      Started:      Fri, 31 Jan 2025 03:04:39 +0000
      Finished:     Fri, 31 Jan 2025 03:04:39 +0000
    Ready:          True
    Restart Count:  8
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-plvsh (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-plvsh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason          Age                   From               Message
  ----     ------          ----                  ----               -------
  Normal   Scheduled       16m                   default-scheduler  Successfully assigned default/mong-deply-659b9dd647-9wqtv to minikube
  Normal   Pulled          16m                   kubelet            Successfully pulled image "mongo" in 9.172s (9.172s including waiting). Image size: 854891519 bytes.
  Normal   Pulled          16m                   kubelet            Successfully pulled image "mongo" in 379ms (379ms including waiting). Image size: 854891519 bytes.
  Normal   Pulled          16m                   kubelet            Successfully pulled image "mongo" in 341ms (341ms including waiting). Image size: 854891519 bytes.
  Normal   Pulled          15m                   kubelet            Successfully pulled image "mongo" in 374ms (374ms including waiting). Image size: 854891519 bytes.
  Normal   Pulled          15m                   kubelet            Successfully pulled image "mongo" in 426ms (426ms including waiting). Image size: 854891519 bytes.
  Normal   Started         13m (x6 over 16m)     kubelet            Started container mongo
  Normal   Created         13m (x6 over 16m)     kubelet            Created container: mongo
  Normal   Pulled          13m                   kubelet            Successfully pulled image "mongo" in 422ms (422ms including waiting). Image size: 854891519 bytes.
  Normal   Pulled          11m                   kubelet            Successfully pulled image "mongo" in 434ms (434ms including waiting). Image size: 854891519 bytes.
  Warning  BackOff         6m30s (x47 over 16m)  kubelet            Back-off restarting failed container mongo in pod mong-deply-659b9dd647-9wqtv_default(51d99b21-a899-4363-b906-979c7a309d1a)
  Normal   Pulling         5m52s (x8 over 16m)   kubelet            Pulling image "mongo"
  Normal   SandboxChanged  2m12s                 kubelet            Pod sandbox changed, it will be killed and re-created.
  Normal   Pulling         2m11s                 kubelet            Pulling image "mongo"
  Normal   Pulled          2m11s                 kubelet            Successfully pulled image "mongo" in 357ms (465ms including waiting). Image size: 854891519 bytes.
  Normal   Created         2m11s                 kubelet            Created container: mongo
  Normal   Started         2m11s                 kubelet            Started container mongo


######################################################################################
## Login to terminal of container 
######################################################################################

kctl exec  -it [pod name] -- bin/bash     ## NOTE:  -it == interactive terminal

Puts you into a bash shell into the specific pod:

kctl exec -it mong-deply-659b9dd647-9wqtv -- bin/bash
root@mong-deply-659b9dd647-9wqtv:/# ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
mongodb        1       0  0 03:08 ?        00:00:02 mongod --bind_ip_all
root          66       0  0 03:18 pts/0    00:00:00 bin/bash
root          79      66  0 03:19 pts/0    00:00:00 ps -ef



######################################################################################
## Delete the Deployment ( and the Replicaset and the Pods )
######################################################################################b
kctl get deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
mong-deply   1/1     1            1           28m
nginx-dep    1/1     1            1           59m

kctl get pod
NAME                          READY   STATUS    RESTARTS      AGE
mong-deply-659b9dd647-9wqtv   1/1     Running   8 (18m ago)   29m
nginx-dep-579d65b68f-7zn5w    1/1     Running   1 (17m ago)   39m

kctl delete deployment mong-deply
deployment.apps "mong-deply" deleted      


NOTE: pod and replicaset are now DELETED

kctl get pod
NAME                         READY   STATUS    RESTARTS      AGE
nginx-dep-579d65b68f-7zn5w   1/1     Running   1 (19m ago)   40m

kctl get replicaset
NAME                   DESIRED   CURRENT   READY   AGE
nginx-dep-579d65b68f   1         1         1       40m
nginx-dep-5879cc9      0         0         0       61m

Now let's get rid of nginx:

kctl delete deployment nginx-dep
deployment.apps "nginx-dep" deleted

kctl get replicaset
No resources found in default namespace.

kctl get pod
No resources found in default namespace.

ALL CRUD OPERATIONS HAPPENS AT DEPLOYMENT LEVEL (replicasets and pods are handled automatically) 



######################################################################################
## Deploy via apply command
######################################################################################b

Deployments can be created via file and executed via:

kubectl apply  -f [file name]


Apply a configuration to a resource by file name or stdin. The resource name must be specified. This resource will be
created if it doesn't exist yet. To use 'apply', always create the resource initially with either 'apply' or 'create
--save-config'.

 JSON and YAML formats are accepted.

Examples:
  # Apply the configuration in pod.json to a pod
  kubectl apply -f ./pod.json
  
  # Apply resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl apply -k dir/
  
  # Apply the JSON passed into stdin to a pod
  cat pod.json | kubectl apply -f -
  
  # Apply the configuration from all files that end with '.json'
  kubectl apply -f '*.json'

## Basic deployment yaml file: nginx-deployment.yaml

apiVersion: apps/vi
kind: Deployment                # Type of resource to create
metadata: 
  name: nginx-deployment        # Deployment name
  labels: 
    app: nginx                  # label
spec:                           # spec for deployment 
  replicas: 1
  selector: 
    matchLabels:
      app: nginx
  template:                                          ------+
    metadata:                                              |
      labels:                                              |
        app: nginx                                         |
    spec:                       # spec for pod             +--- blueprint for pod
      containers:                                          |
        - name: nginx                                      |
          image: nginx:1.16                                |
          ports:                                           |
            - containerPort: 80                      ------+

## Now apply yaml config file 

kctl apply -f nginx-deployment.yaml 
deployment.apps/nginx-deployment created

kctl get deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   1/1     1            1           7s

kctl get replicaset
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-7f65fcf556   1         1         1       12s

kctl get pod
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-7f65fcf556-d9zcr   1/1     Running   0          16s


You can delete the same way: 

kctl delete  -f nginx-deployment.yaml 

SUMMARY: 

CRUD 
  kctl create deployment [name]
  kctl edit   deployment [name]
  kctl delete deployment [name]

Status
  kctl get nodes | pod | services | replicaset | deployment

Debug
  kctl logs [pod name]                    ## Log to console 
  kctl -exec -it [pod name] -- bin/bash   ## Get interatcive terminal into pod
  kctl describe [pod name]                ## Get information about pod

CRUD by file 
  kctl apply  -f [file name]
  kctl delete -f [file name]




######################################################################################
## K8s YAML and Conig File
######################################################################################

- connect Deployments to Service to Pods



- 3 parts of a config file
  1. Metadata:  name, labels etc
  2. Specification:  configuration for the component
  3. Status: generated and added by k9s
    - desired state (replicas 2) vs. actual state
    If desired != actual then k8s will try to fix 
      - i.e self-healing cluster
      - updates state continuously 

NOTE: etcd holds status / updates     

## nginx-deployment.yaml
## 
apiVersion: apps/v1
kind: Deployment           <<---- Kind of component
metadata:                  <<---- Metadata 
  name: nginx-deployment            - name
  labels:
spec:                      <<---- Metadata
  repilcas: 2
  selector:
  template:



## nginx-service.yaml
## 
apiVersion: v1
kind: Service              <<---- Kind of component
metadata:                  <<---- Metadata
  name: nginx-service               - name
  labels:
spec:                      <<---- Metadata
  selector:
  ports:


## YAML config file format
  - human friendly / strict on indendentation
    NOTE: yamllint.com, online validator

- store yaml with code 



template: 
 - has own metadata and spec section
 - template configuation applies to a pod 
 - blueprint for a pod
    - image / port / container name



template: 
  metadata: 
    labels: 
      app: nginx 
  spec:
    containers: 
    - name: nginx  
      image: nginx:1.16  
      ports: 
      - containerPort: 8080

# connecting components:  labels and selectors and ports
- labels and selectors make the "connection" between deployment and service 
  - metadata contains labels 
  - spec     contains selectors 
ex. 



## nginx-deployment.yaml
## 
apiVersion: apps/v1
kind: Deployment           
metadata:                  
  name: nginx-deployment           
  labels:
    app: nginx             <<---- Deployment label (Used by Service Selector)
spec:                      
  repilcas: 2
  selector:                <<---- Selector matches the pod label 
    
      app: nginx
  template:       ------+
    metadata:           |
      labels:           +--- NOTE: pods get label thru template blueprint
        app: nginx      |
                  ------+


## nginx-service.yaml
## 
apiVersion: v1
kind: Service              
metadata:                  
  name: nginx-service      
  labels:
spec:                      
  selector:      -------+
    app: nginx          |-------- Service Selector matches Service to Deployment 
  ports:         -------+          (matches service - selector to Deployment label, and pods declares in template )




       Deployment                                                 Service 
---------------------------                                  -------------------
apiVersion: apps/v1                                           apiVersion: v1
kind: Deployment                                              kind: Service
metadata:                                                     metadata:
  name: nginx-deployment                                        name: nginx-service
  labels:                                                     spec:
     app: nginx                                                 selector:
spec:                                   Service selector  +---->  app: nginx
  replicas: 2                          matches pod label  |     ports:
  selector:                                               |       - protocol: TCP
    matchLabels:                                          |         port: 80  <---------Service external port
      app: nginx    <-------------+                       |        targetPort: 8080
  template:                       | Deployment uses       |                      ^
    metadata:                     + matchLabels to match  |                      |
      labels:                     | pod template label    |                      |  Service target port
        app: nginx  <-------------+                       |                      |  matches container port
    spec:       ^+----------------------------------------+                      |
      containers:                                                                |
      - name: nginx                                                              |
        image: nginx:1.16                                                        |
        ports:                                                                   |
        - containerPort: 8080   <------------------------------------------------+
                                 


# Connecting Deployment to Pods
  - component has a key value pair
  i.e.  labels:
          app: nginx   key/valu pair associated with component

The pod (via template) gets a label :  app: nginx 
This is the same label that is in the Deployment label
The spec: of the Deployment determines matches key/values app: nginx in the Deployment and template 
spec:
  replicas: 2 
  selector: 
    matchLabels:      << Match the deployment to the pod template 
      app: nginx


## Ports 

## nginx-service.yaml
## 
apiVersion: v1
kind: Service              
metadata:                  
  name: nginx-service      
  labels:
spec:                      
  selector:      
    app: nginx   
  ports:
    - protocol: TCP         
      port: 80            NOTE: port is the port that the Service respons externally on 
      targetPort: 8080    NOTE: targetPort is the port of the Pod (container) to connect to 



## nginx-deployment.yaml
## 
apiVersion: apps/v1
kind: Deployment           
.... 
  template:       
    metadata:      
      labels:       
        app: nginx   
    spec:      
      containers: 
      - name: nginx    
        image: nginx:1.16 
        ports: 
        - containerPort: 8080  NOTE: containerPort is linked to Service targetPort
                  

NOTE: The service / deploy files are in k8s_review/deploy_service/ folder
      (copied to minikube)

kubectl apply -f nginx-deployment.yaml
kubectl apply -f nginx-service.yaml
kubectl get pod
kubectl get service
kubectl describe service nginx-service

kctl describe service nginx-service
Name:                     nginx-service
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=nginx
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.110.168.194
IPs:                      10.110.168.194
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
Endpoints:                10.244.0.25:8080,10.244.0.26:8080  <---------------+
Session Affinity:         None                                               |
Internal Traffic Policy:  Cluster                                            |
Events:                   <none>                                          matches 
                                                                             |
## get pod information to match up with Endpoints above                      |
kctl get pod -o wide                                                         |
                                                                             v
NAME                                READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
nginx-deployment-687d5fb844-6q6js   1/1     Running   0          4m53s   10.244.0.26   minikube   <none>           <none>
nginx-deployment-687d5fb844-hnpv6   1/1     Running   0          4m53s   10.244.0.25   minikube   <none>           <none>


## get the status of the deployment  ( this is in etc )
kctl get deployment nginx-deployment -o yaml

ApiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"nginx"},"name":"nginx-deployment","namespace":"default"},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"nginx"}},"template":{"metadata":{"labels":{"app":"nginx"}},"spec":{"containers":[{"image":"nginx:1.16","name":"nginx","ports":[{"containerPort":8080}]}]}}}}
  creationTimestamp: "2025-02-03T00:26:09Z"
  generation: 1
  labels:
    app: nginx
  name: nginx-deployment
  namespace: default
  resourceVersion: "23248"
  uid: d6e4a9ae-8482-4c12-ba91-c498725fb215
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.16
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-02-03T00:26:10Z"
    lastUpdateTime: "2025-02-03T00:26:10Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2025-02-03T00:26:09Z"
    lastUpdateTime: "2025-02-03T00:26:10Z"
    message: ReplicaSet "nginx-deployment-687d5fb844" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 2
  replicas: 2
  updatedReplicas: 2



# Can delete the Deployment and Service using the same yaml files 

kubectl delete -f nginx-deployment.yaml
kubectl delete -f nginx-service.yaml

## Demo Apps 
MongoDB , mongo-express
 2 Deployments / pod
 2 Service
 1 ConfigMap 
 1 Secret


Internal Service   [ConfigMap]                      Deployment.yaml
                   [ DB URL  ]                      (Env Variables)

[MongoDB ] <---------------------------------------- [MongoExpress] <------ [ External Service ]
                    [Secret ]
                    [DB User]
                    [DB Pass]

## Anatomy of a Secret File
apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: dXNlcm5hbWU=   <<--= BASE64 Encoded
    mongo-root-password: cGFzc3dvcmQ=   <<--= BASE64 Encoded

# how to get base64
 echo -n 'username' | base64



NOTE: see code in k8s_review/mongodb_app
      mongo-secret.yaml MUST BE DEPLOYED BEFORE  mongo or mongo-express because they reference it.
      Also, Deployment and Service YAML are together in one file 


### kubectl apply commands in order,   
### NOTE: ORDER IS IMPORTANT.  Must deploy resources (i.e. secrets) BEFORE
###       they can be referenced in Deployments 
    
    kubectl apply -f mongo-secret.yaml
    kubectl apply -f mongo.yaml
    kubectl apply -f mongo-configmap.yaml 
    kubectl apply -f mongo-express.yaml

### kubectl get commands

    kubectl get pod
    kubectl get pod --watch
    kubectl get pod -o wide
    kubectl get service
    kubectl get secret
    kubectl get all | grep mongodb

### kubectl debugging commands

    kubectl describe pod mongodb-deployment-xxxxxx
    kubectl describe service mongodb-service
    kubectl logs mongo-express-xxxxxx

### give a URL to external service in minikube

    minikube service mongo-express-service


Anatomy of a mongo-express load balancer

---
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express
  type: LoadBalancer         << type =- LoadBalancer 
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
      nodePort: 30000       << nodePort: ephemeral port for node, between 30000 - 32767

Port for external IP address


  
kctl get service
NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes              ClusterIP      10.96.0.1       <none>        443/TCP          6d1h
mongo-express-service   LoadBalancer   10.110.253.68   <pending>     8081:30000/TCP   11m
mongodb-service         ClusterIP      10.99.39.17     <none>        27017/TCP        23m

NOTE: once everything is deployed, the mongo-express-service EXTERNAL-IP address is still <pending>.
Have to take an extra setup with minikube (otherwise an External-IP would have been configured)
 > minikube service mongo-express-service

minikube service mongo-express-service
|-----------|-----------------------|-------------|---------------------------|
| NAMESPACE |         NAME          | TARGET PORT |            URL            |
|-----------|-----------------------|-------------|---------------------------|
| default   | mongo-express-service |        8081 | http://192.168.49.2:30000 |
|-----------|-----------------------|-------------|---------------------------|
🎉  Opening service default/mongo-express-service in default browser...
👉  http://192.168.49.2:30000

[External IP:30000] 
    |
    +--->> [ Mongo Express External Service ] 
                |
                +--->> [ Mongo Express Pod ] 
                            |
                            +--->> [ Mongo DB Internal Service ]
                                           |
                                           +--->> [ Mongo DB Pod ]



######################################################################################
## K8s Namespaces
######################################################################################

Namespace 
 - can organize resources in namespace 
 - think of as a virtual cluster within k8s

4 namespaces by default

kctl get namespace
NAME                   STATUS   AGE
default                Active   6d11h
kube-node-lease        Active   6d11h
kube-public            Active   6d11h
kube-system            Active   6d11h
kubernetes-dashboard   Active   6d11h   
     +----- << Shipped with minikube
                                                                +----------k8s------------+
                                                                | namespace:              |
1. kube-system:                                                 |                         |  
  - do not crate / modify kube-system                           | [kube-system]           |
  - system processes                                            |                         |
  - Master and kubectl processes                                | [kube-public]           |
                                                                |                         |
2. kube-public:                                                 | [kube-node-lease]       |
  - publicly accessible data                                    |                         |
  - config map that contains cluster informatiion               | [default]               |                                 |
     - accessible without authenticatio                         |                         |
  i.e. kubectl cluster-info                                     +-------------------------+

Kubernetes control plane is running at https://192.168.49.2:8443
CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

3. kube-node-lease:  
  - holds info of heartbeats of nodes
  - each node has associated lease object in namespace
  - determines availability of a node

4. default
  - default location for creating resources 

# Create a namespace via:
  kubectl create namespace map-nmspace 
  kubectl get namespaces 

# Create a namespace fia config file:

mynamespace-configmap.yaml
------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-configmap
  namespace: my-namespace
data:
  db_url: mysql-service.database

# Why use a namespace
1. Organize resources
  - resources become unorganized quickly
  - allows for grouping of related resources

+------------------k8s--------------------+
| +-[database]---+      +-[monitoring]-+  |
| | [ ]    [ ]   |      |     [ ]      |  |
| |              |      | [ ]    [ ]   |  |
| +--------------+      +--------------+  |
|                                         |
|                                         |
| +-[elastic stack]-+   +[nginx-ingress]+ |
| |                 |   |  [ ]    [ ]   | |
| | [ ]     [ ]     |   |      [ ]      | |
| +-----------------+   +---------------+ |
|                                         |
+------------------k8s--------------------+

NOTE: K8s documentations states that SHOULD NOT use namespaces 
  for small projects, up to 10 users. YMMV


2. Avoid conflicts from multiple teams
  team1: my-app Deployment in default
  team2: my-app Deployment in default
    - team2 OVERWRITE team1 deployment

- using namespaces segregates the two teams

  
+------------------k8s--------------------+
|                                         |
| +-[team1-ns]---+      +-[team2-ns]---+  |
| | [ my-app ]   |      | [ my-app ]   |  |       
| |              |      | [ my-app ]   |  |
| +--------------+      +--------------+  |
|                                         |
+------------------k8s--------------------+


3. Resource sharing:  staging / dev
   - don't need to setup separate clusters for Stage / Dev

+--------------------k8s-----------------------+
|                                              |
|  +-[Staging] ---+        +-[Dev]--------+    |
|  | [ my-app ]   |        | [ my-app ]   |    |       
|  |              |        | [ my-app ]   |    |
|  +--------------+        +--------------+    |
|         |                       |            |
|         +------------+----------+            |
|                      |                       |
|  (reuse components   |  for stage and dev)   |
|         +------------+----------+            |
|         |                       |            |
|  +-[Nginx-Ingress]-+    +-[Elastic Stack]-+  |    
|  | [ .. ]          |    | [ my-app ]      |  |    
|  |        [ .. ]   |    |                 |  |    
|  +-----------------+    +-----------------+  |    
|                                              |
+----------------------k8s---------------------+


3a. Blue-Green Deployment
   - don't need to setup separate cluster for Blue/Green

+--------------------k8s-----------------------+
|                                              |
|  +-[PROD-Blue]--+        +-[PROD-Green]-+    |
|  | [ my-app ]   |        | [ my-app ]   |    |       
|  |              |        | [ my-app ]   |    |
|  +--------------+        +--------------+    |
|         |                       |            |
|         +------------+----------+            |
|                      |                       |
|                      |                       |
|         +------------+----------+            |
|         |                       |            |
|  +-[Nginx-Ingress]-+    +-[Elastic Stack]-+  |    
|  | [ .. ]          |    | [ my-app ]      |  |    
|  |        [ .. ]   |    |                 |  |    
|  +-----------------+    +-----------------+  |    
|                                              |
+----------------------k8s---------------------+


4. Access and Resource Limits on Namespaces (w/ Multiple Teams)
   - Teams can only access / update their own namespace
   - Limit CPU, RAM, Storage per Namespace
        - via Resource Quota

               +--------------------k8s-----------------------+
               |                                              |
               |  +-[Project-A]--+        +-[Project-B]--+    |
               |  | [ my-app ]   |        | [ my-app ]   |    |       
               |  |              |        | [ my-app ]   |    |
               |  +--------------+        +--------------+    |
               | [Resource Quota A]      [Resource Quota B]   |
               | [2CPU 16GB RAM...]      [1CPU 8GB RAM ...]   |
               |                                              |
               +----------------------k8s---------------------+


# Characteristics of a Namespace
 - Can't access most resources from another Namespace
  - Each Namespace MUST DEFINE own ConfigMap
  - Each Namespace MUST DEFINE own Secret (even for shared service)

 - Can access a service i.e.

apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-configmap
  namespace: my-namespace
data:
  db_url: mysql-service.database
                |          |
  name ---------+          +-------- namespace

               +--------------------k8s-----------------------+
               |                                              |
               |  +-[Project-A]--+        +-[Project-B]--+    |
               |  | [ my-app ]   |        | [ my-app ]   |    |       
               |  |              |        | [ my-app ]   |    |
               |  +--------------+        +--------------+    |
               |                                              |
               |           +--[database]------+               |
               |           |                  |               |
               |           | [mysql-service]  |               |
               |           |                  |               |
               |           +------------------+               |
               |                                              |
               +----------------------k8s---------------------+


# K8s Components NOT in namespace
 - live globally in a cluster
 - can't isolate them

               +--------------------k8s-----------------------+
               |                                              |
               |  +-[Project-A]--+        +-[Project-B]--+    |
               |  | [ my-app ]   |        | [ my-app ]   |    |       
               |  |              |        | [ my-app ]   |    |
               |  +--------------+        +--------------+    |
               |                                              |
               |     [ ------ ]           [ ------ ]          |      
               |     [ Volume ]           [  Node  ]          |
               |     [ ------ ]           [ ------ ]          |
               |                                              |
               +----------------------k8s---------------------+

# list NON-namespaced resources
kubectl api-resource --namespaced=falee


kctl api-resources --namespaced=false
NAME                                SHORTNAMES   APIVERSION                        NAMESPACED   KIND
namespaces                          ns           v1                                false        Namespace
nodes                               no           v1                                false        Node
persistentvolumes                   pv           v1                                false        PersistentVolume
componentstatuses                   cs           v1                                false        ComponentStatus
....
apiservices                                      apiregistration.k8s.io/v1         false        APIService
certificatesigningrequests          csr          certificates.k8s.io/v1            false        CertificateSigningRequest
csidrivers                                       storage.k8s.io/v1                 false        CSIDriver
csinodes                                         storage.k8s.io/v1                 false        CSINode
storageclasses                      sc           storage.k8s.io/v1                 false        StorageClass
volumeattachments                                storage.k8s.io/v1                 false        VolumeAttachment


# list namespaced resources

kctl api-resources --namespaced=true
NAME                        SHORTNAMES   APIVERSION                     NAMESPACED   KIND
bindings                                 v1                             true         Binding
configmaps                  cm           v1                             true         ConfigMap
deployments                 deploy       apps/v1                        true         Deployment
endpoints                   ep           v1                             true         Endpoints
events                      ev           v1                             true         Event
persistentvolumeclaims      pvc          v1                             true         PersistentVolumeClaim
pods                        po           v1                             true         Pod
replicasets                 rs           apps/v1                        true         ReplicaSet
resourcequotas              quota        v1                             true         ResourceQuota
secrets                                  v1                             true         Secret
serviceaccounts             sa           v1                             true         ServiceAccount
services                    svc          v1                             true         Service
....
statefulsets                sts          apps/v1                        true         StatefulSet



## Create a component in a namespace 
 - no namespace defined, created in default namespace
 - if you don't pass a namespace to kubectl it will use default
      i.e. kctl get configmap == kctl get configmap -n default
1. pass --namespace parameter to apply 

  kubectl apply -f mynamespace-configmap.yaml --namespace=map-ns


2. configure namespace: attribute in ConfigMap yaml file

apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-configmap
  namespace: my-namespace
data:
  db_url: mysql-service.database

NOTE: use -n option to get for that namespace 

kubectl get configmap -n map-ns


BEST PRACTICE:  configure namespace attribute in yaml and not via kubectl --namespace
  this way the configuration is captured IN THE CODE



## Change an active namespace 
kubens (separate tool )
NOTE: installed via: 
         sudo snap install kubectx --classic

kubens    # highlights active namespace and all others
default
kube-node-lease
kube-public
kube-system
kubernetes-dashboard
map-nmspace

kubens map-nmspace  # switch from default to map-nmspace

nsible@minikube:~/mongo_demo$ kubens map-nmspace
✔ Active namespace is "map-nmspace"



######################################################################################
## K8s Ingress
######################################################################################


                             +-------- https://124.89.101.2:35010
                             |          (ip address of node and node port)
                             |
    +----k8s-----------------+-------------------+
    |                        |                   |
    |                        |                   |
    |                [external service]          |
    |                        |                   |
    |                        v                   |
    |  [myapp pod]<-- [myapp service] <--+       |
    |                                    |       |   +-------- https://myapp.com
    |                                    |       |   |
    |                      [ myapp ingress  ] <--+---+
    |                      [internal service]    |
    |                                            |
    +--------------------------------------------+

# YAML config external vs. ingress service 

# external service YAML
# 
apiVersion: v1
kind: Service                    <<------------------------------------------+
metadata:                                                                    |
  name: myapp-external-service                                               |
spec:                                                                        |
  selector:                                                                  |
    app: myapp                                                               |
  type: LoadBalancer      ## NOTE: type: LoadBalancer makes this an External Service 
  ports:
    - protocol: TCP  
      port: 8080  
      targetPport: 8080  
      nodePort: 35010     ## NOTE: the port of the node that will handle this traffic


# Ingress service YAML
# 
apiVersion: networking.k8s.io/v1beta1
kind: Ingress                    <<---------+ ## NOTE:  Ingress Service with Routing Rules
metadata:                                   |
  name: myapp-ingress                       |
spec:                                       |
  rules:                                    | 
  - host: myapp.com              <<---------+-- host: name mapped to service 
    http:                                   |   http:  NOT incoming http request from browser but forward to internal service
      paths:                                |   path: URL path: /reports  /confg /etc
      - backend:                            v
          serviceName: myapp-internal-service  <<---------+
          servicePort: 8080                               |
                                                          |
                                                          |
# Internal Service config                                 +-------  Ingress backend.serviceName 
#                                                         |                 backend.servicePort
apiVersion: v1                                            |          maps  
kind: Service                                             |          to
metadata:                                                 |            
  name: myapp-internal-service    <<----------------------+         Internal service.metadata.name
spec:                                                               service.spec.ports.port 
  selector:                                               
    app: myapp                                            
  ports:
    - protocol: TCP  
      port: 8080  
      targetPport: 8080  


NOTE:  - ingress.spec.rules.host should be a valid domain address (myapp.com)
       - domain name SHOULD BE mapped to Node's IP address, which is the entry point   

# Configuring Ingress in k8s cluster

The below is not enough,  Need an "implementation" for Ingress >> Ingress Controller 

 [pod: mayapp] <- [svc: myapp] <- [ingress: myapp] <- [pod: ingress controller]

ingress controller: evaluates and processes Ingress rules
  - manages redirections
  - entrypoint to cluster 
  - many 3rd party implementations i.e. K8s Nginx Ingress Controller, etc

# ingress controllers 
https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/

# bare metal ingress controller 
  https://kubernetes.github.io/ingress-nginx/deploy/baremetal/


Environment where k8s is running: 

# Cloud Provider
 [Cloud Load Balancer] -----------------------------------------+
                                                                | 
                                                                v
 [pod: mayapp] <- [svc: myapp] <- [ingress: myapp] <- [pod: ingress controller]

# Bare Metal:  Need to configure and entry point 
                - inside cluster or outside separate server
                
Proxy Server:  
 - separate server (HW/SW) 
 - public IP address and open ports 
 - acts as entrypoint to cluster
NOTE:  No server in K8S cluster is accessible from outside !!

 [Proxy Server] ------------------------------------------------+
                                                                | 
                                                                v
 [pod: mayapp] <- [svc: myapp] <- [ingress: myapp] <- [pod: ingress controller]


## Minikube Ingress

## Start the Ingress Controller 
## 
minikube addons enable ingress  # Starts K8s Nginx Ingress Controller

minikube addons enable ingress
💡  ingress is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub.
You can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS
    ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4
    ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4
    ▪ Using image registry.k8s.io/ingress-nginx/controller:v1.11.3
🔎  Verifying ingress addon...
🌟  The 'ingress' addon is enabled

# Here is ingress-nginx info
kubens ingress-nginx
✔ Active namespace is "ingress-nginx"

kctl get pods
NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-rlfhx        0/1     Completed   0          91s
ingress-nginx-admission-patch-knw2v         0/1     Completed   0          91s
ingress-nginx-controller-56d7c84fd4-5rg5t   1/1     Running     0          91s

kctl get deployment
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
ingress-nginx-controller   1/1     1            1           100s

kctl get service
NAME                                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.97.117.165   <none>        80:32342/TCP,443:32031/TCP   103s
ingress-nginx-controller-admission   ClusterIP   10.107.37.144   <none>        443/TCP                      103s


## Now create an Ingress Rule
## 

# Create an ingress to dashboard (so that an ingress rule can be configured ?)
# (see democode/kubernetes-ingress/dashboard-ingress.yaml)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
spec:
  ingressClassName: "nginx"
  rules:
  - host: dashboard.com
    http:
      paths:
      - path: /
        pathType: Prefix  
        backend:
          service:
            name: kubernetes-dashboard
            port: 
              number: 80

kctl get ingress -n kubernetes-dashboard
NAME                CLASS   HOSTS           ADDRESS        PORTS   AGE
dashboard-ingress   nginx   dashboard.com   192.168.49.2   80      11s

#NOTE add 192.168.49.2 dashboard to /etc/hosts then run  
curl dashboard.com

## describe the ingress 
## 
kctl describe ingress dashboard-ingress
Name:             dashboard-ingress
Labels:           <none>
Namespace:        kubernetes-dashboard
Address:          192.168.49.2
Ingress Class:    nginx
Default backend:  <default>
Rules:
  Host           Path  Backends
  ----           ----  --------
  dashboard.com  
                 /   kubernetes-dashboard:80 (10.244.0.28:9090)
Annotations:     <none>
Events:
  Type    Reason  Age                From                      Message
  ----    ------  ----               ----                      -------
  Normal  Sync    23m (x2 over 24m)  nginx-ingress-controller  Scheduled for sync

NOTE: Default backend handles requests that are not mapped via path





[ 2:01:53 ]
Hashicorp Vault
https://www.youtube.com/watch?v=klyAhaklGNU

.